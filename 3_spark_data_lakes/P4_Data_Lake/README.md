# Project: Data Lake

## Project Description

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app.

The goal of this project is to build an ETL pipeline that extracts their data from S3, processes them using Spark on EMR, and loads the data back into S3 as a set of dimensional tables.

## How to run the project
### AWS EMR Cluster
1. Create an EMR cluster with Spark on AWS with appropriate number of core and task nodes(optional).
2. Once the cluster is provisoned connect to the master node using either ssh or Session manager.
3. Create an S3 bucket in the same region as EMR cluster where the output files will be stored.
4. Run the etl.py by using the below command
```
/usr/bin/spark-submit etl.py
```
### Running on local machine
1. To run this project in local mode*, create a file `dl.cfg` in the root of this project with the following data:

```
KEY=YOUR_AWS_ACCESS_KEY
SECRET=YOUR_AWS_SECRET_KEY
```
2. Create an S3 bucket in the same region as EMR cluster where the output files will be stored.
3. Run the etl.py script using command - python etl.py

## Project files

1. **etl.py** Reads and processes all the files from S3 bucket, and loads the data back to an S3 bucket


## Project Datasets

We'll be working with two datasets that reside in S3. Here are the S3 links for each:
- Song data: ```s3://udacity-dend/song_data```
- Log data: ```s3://udacity-dend/log_data```

#### Song DataSet
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
```song_data/A/B/C/TRABCEI128F424C983.json
   song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
```{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}```

#### Log DataSet
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.
The log files in the dataset we'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```log_data/2018/11/2018-11-12-events.json
   log_data/2018/11/2018-11-13-events.json
```

## Database Schema

#### Fact Table
- **songplays** - Records from event data which simulate app activity logs (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location,user_agent)

#### Dimension Tables
- **users** - Users of the app (user_id, first_name, last_name, gender, level)
- **songs** - songs in the song database (song_id, title, artist_id, year, duration)
- **artists** - Artists for the songs in the song database (artist_id, name, location, lattitude, longitude)
- **time** - Timestamp of event data broken into specific units (start_time, hour, day, week, month, year, weekday)

## Detailed description of ETL pipeline

1. We read the data from S3
2. Data is processed using pyspark to map it into dimension and fact tables with right datatypes i.e we do ETL
3. Data is written back to S3 in parquet format with proper partitioning.
4. Each of the table is written into different prefix in S3 bucket
4. Song data is partitioned by year and artist while Time and Songplays are  partitioned by year and month.



